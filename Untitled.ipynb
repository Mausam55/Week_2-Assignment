{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0742444-b2bf-499f-867d-a60ba93f08c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\L E N O V\n",
      "[nltk_data]     O\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\L E N O V\n",
      "[nltk_data]     O\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\L E N O V\n",
      "[nltk_data]     O\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download (\"stopwords\")\n",
    "nltk.download (\"punkt\")\n",
    "nltk.download (\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590a4808-e802-40dc-a360-8889be918135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba11095-bf78-4e5f-b7c9-1ed5d96b7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "\n",
    "    def __init__(self, directory_path):\n",
    "        self.directory_path = directory_path\n",
    "        self.documents = {}\n",
    "        self.doc_mapping = {}\n",
    "\n",
    "    def load_documents(self):\n",
    "        print(f\"Loading documents from: {self.directory_path}\")\n",
    "        doc_counter = 0\n",
    "\n",
    "        for file in os.listdir(self.directory_path):\n",
    "            print(f\"Processing: {file}\")\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(self.directory_path, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text_content = f.read()\n",
    "                    self.documents[doc_counter] = text_content\n",
    "                    self.doc_mapping[doc_counter] = file\n",
    "                    print(f\"Document ID {doc_counter} mapped to {file}\")\n",
    "                    doc_counter += 1\n",
    "\n",
    "        print(f\"Successfully loaded {len(self.documents)} documents\")\n",
    "        return self.documents, self.doc_mapping\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if len(token) > 1]\n",
    "        return processed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55edeeb-1772-44f2-ac4a-734baa9f827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexBuilder:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(set)\n",
    "        self.term_stats = Counter()\n",
    "\n",
    "    def create_index(self, documents, processor):\n",
    "        for doc_id, text in documents.items():\n",
    "            tokens = processor.preprocess_text(text)\n",
    "            for token in tokens:\n",
    "                self.index[token].add(doc_id)\n",
    "                self.term_stats[token] += 1\n",
    "\n",
    "        return self.index, self.term_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2a5b5d-2b0c-4862-b7ed-abf77cd611b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(self, query_string):\n",
    "    query_string = query_string.lower()\n",
    "    query_tokens = query_string.split()\n",
    "    \n",
    "    matching_docs = set()\n",
    "    \n",
    "    operators = ['and', 'or', 'not']\n",
    "    search_terms = [token for token in query_tokens if token not in operators]\n",
    "    \n",
    "    if 'and' in query_tokens:\n",
    "        if all(term in self.inverted_index for term in search_terms):\n",
    "            matching_docs = self.inverted_index[search_terms[0]].copy()\n",
    "            for term in search_terms[1:]:\n",
    "                matching_docs &= self.inverted_index[term]\n",
    "    \n",
    "    elif 'or' in query_tokens:\n",
    "        for term in search_terms:\n",
    "            if term in self.inverted_index:\n",
    "                matching_docs |= self.inverted_index[term]\n",
    "    \n",
    "    elif 'not' in query_tokens:\n",
    "        excluded_term = query_tokens[1]\n",
    "        all_document_ids = set(self.doc_mapping.keys())\n",
    "        if excluded_term in self.inverted_index:\n",
    "            matching_docs = all_document_ids - self.inverted_index[excluded_term]\n",
    "        else:\n",
    "            matching_docs = all_document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a93fc32-37c4-400e-b021-3bc74c4cf561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_file(term_statistics, output_path=\"queries.txt\", num_queries=5):\n",
    "    sample_queries = [\n",
    "        \"update AND feature\",\n",
    "        \"android OR window\",\n",
    "        \"NOT support\"\n",
    "    ]\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as query_file:\n",
    "        for q in sample_queries:\n",
    "            query_file.write(q + \"\\n\")\n",
    "    \n",
    "    print(f\"Query file created: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f605c1b-390e-4f72-99fe-69edf909cf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from: C:\\Users\\L E N O V O\\anaconda3\\Lib\\site-packages\\nltk\n",
      "Processing: app\n",
      "Processing: book.py\n",
      "Processing: ccg\n",
      "Processing: chat\n",
      "Processing: chunk\n",
      "Processing: classify\n",
      "Processing: cli.py\n",
      "Processing: cluster\n",
      "Processing: collections.py\n",
      "Processing: collocations.py\n",
      "Processing: compat.py\n",
      "Processing: corpus\n",
      "Processing: data.py\n",
      "Processing: decorators.py\n",
      "Processing: downloader.py\n",
      "Processing: draw\n",
      "Processing: featstruct.py\n",
      "Processing: grammar.py\n",
      "Processing: help.py\n",
      "Processing: inference\n",
      "Processing: internals.py\n",
      "Processing: jsontags.py\n",
      "Processing: langnames.py\n",
      "Processing: lazyimport.py\n",
      "Processing: lm\n",
      "Processing: metrics\n",
      "Processing: misc\n",
      "Processing: parse\n",
      "Processing: probability.py\n",
      "Processing: sem\n",
      "Processing: sentiment\n",
      "Processing: stem\n",
      "Processing: tag\n",
      "Processing: tbl\n",
      "Processing: test\n",
      "Processing: text.py\n",
      "Processing: tgrep.py\n",
      "Processing: tokenize\n",
      "Processing: toolbox.py\n",
      "Processing: translate\n",
      "Processing: tree\n",
      "Processing: treeprettyprinter.py\n",
      "Processing: treetransforms.py\n",
      "Processing: twitter\n",
      "Processing: util.py\n",
      "Processing: VERSION\n",
      "Processing: wsd.py\n",
      "Processing: __init__.py\n",
      "Processing: __pycache__\n",
      "Successfully loaded 0 documents\n",
      "Index preview: []\n",
      "Query file created: queries.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BooleanSearchEngine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m             output\u001b[38;5;241m.\u001b[39mwrite(output_line)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 34\u001b[0m     run_search_system()\n",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mrun_search_system\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex preview:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(inverted_index\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m20\u001b[39m])\n\u001b[0;32m     14\u001b[0m create_query_file(term_stats)\n\u001b[1;32m---> 17\u001b[0m search_engine \u001b[38;5;241m=\u001b[39m BooleanSearchEngine(inverted_index, doc_mapping)\n\u001b[0;32m     18\u001b[0m test_queries \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate AND feature\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mandroid OR window\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT support\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m ]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_results.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BooleanSearchEngine' is not defined"
     ]
    }
   ],
   "source": [
    "def run_search_system():\n",
    "    docs_folder = r\"C:\\Users\\L E N O V O\\anaconda3\\Lib\\site-packages\\nltk\"\n",
    "    doc_processor = DocumentProcessor(docs_folder)\n",
    "    documents, doc_mapping = doc_processor.load_documents()\n",
    "    \n",
    "    for doc_id, content in documents.items():\n",
    "        tokens = doc_processor.preprocess_text(content)\n",
    "        print(f\"Document {doc_id} preview:\", tokens[:20])\n",
    "    \n",
    "    index_builder = InvertedIndexBuilder()\n",
    "    inverted_index, term_stats = index_builder.create_index(documents, doc_processor)\n",
    "    print(\"Index preview:\", list(inverted_index.keys())[:20])\n",
    "   \n",
    "    create_query_file(term_stats)\n",
    "    \n",
    "  \n",
    "    search_engine = BooleanSearchEngine(inverted_index, doc_mapping)\n",
    "    test_queries = [\n",
    "        \"update AND feature\",\n",
    "        \"android OR window\",\n",
    "        \"NOT support\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    with open(\"search_results.txt\", 'w', encoding='utf-8') as output:\n",
    "        for query in test_queries:\n",
    "            results = search_engine.execute_query(query)\n",
    "            output_line = f\"Query: '{query}' => Results: {results}\\n\"\n",
    "            print(output_line)\n",
    "            output.write(output_line)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_search_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655a967-a71d-4c5e-87b3-e9656718c020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
